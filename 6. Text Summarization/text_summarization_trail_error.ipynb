{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S. No.</th>\n",
       "      <th>Spec #</th>\n",
       "      <th>Spec Name</th>\n",
       "      <th>Para</th>\n",
       "      <th>Sub Section Heading</th>\n",
       "      <th>Submittal Type</th>\n",
       "      <th>Submittal Description</th>\n",
       "      <th>Target Date</th>\n",
       "      <th>Subcontractor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>024119</td>\n",
       "      <td>SELECTIVE DEMOLITION</td>\n",
       "      <td>1.10-A</td>\n",
       "      <td>WARRANTY</td>\n",
       "      <td>Warranty</td>\n",
       "      <td>Existing Warranties : Remove, replace, patch, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>024119</td>\n",
       "      <td>SELECTIVE DEMOLITION</td>\n",
       "      <td>1.10-B</td>\n",
       "      <td>WARRANTY</td>\n",
       "      <td>Warranty</td>\n",
       "      <td>Notify warrantor on completion of selective de...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>024119</td>\n",
       "      <td>SELECTIVE DEMOLITION</td>\n",
       "      <td>1.5-A</td>\n",
       "      <td>PREINSTALLATION MEETINGS</td>\n",
       "      <td>Meetings</td>\n",
       "      <td>Predemolition Conference : Conduct conference ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>024119</td>\n",
       "      <td>SELECTIVE DEMOLITION</td>\n",
       "      <td>1.6-A</td>\n",
       "      <td>INFORMATIONAL SUBMITTALS</td>\n",
       "      <td>Measurements</td>\n",
       "      <td>Proposed Protection Measures : Submit report, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>024119</td>\n",
       "      <td>SELECTIVE DEMOLITION</td>\n",
       "      <td>1.6-B</td>\n",
       "      <td>INFORMATIONAL SUBMITTALS</td>\n",
       "      <td>Schedules</td>\n",
       "      <td>Schedule of Selective Demolition Activities : ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S. No.  Spec #             Spec Name    Para       Sub Section Heading  \\\n",
       "0      44  024119  SELECTIVE DEMOLITION  1.10-A                  WARRANTY   \n",
       "1      45  024119  SELECTIVE DEMOLITION  1.10-B                  WARRANTY   \n",
       "2      36  024119  SELECTIVE DEMOLITION   1.5-A  PREINSTALLATION MEETINGS   \n",
       "3      37  024119  SELECTIVE DEMOLITION   1.6-A  INFORMATIONAL SUBMITTALS   \n",
       "4      38  024119  SELECTIVE DEMOLITION   1.6-B  INFORMATIONAL SUBMITTALS   \n",
       "\n",
       "  Submittal Type                              Submittal Description  \\\n",
       "0       Warranty  Existing Warranties : Remove, replace, patch, ...   \n",
       "1       Warranty  Notify warrantor on completion of selective de...   \n",
       "2       Meetings  Predemolition Conference : Conduct conference ...   \n",
       "3   Measurements  Proposed Protection Measures : Submit report, ...   \n",
       "4      Schedules  Schedule of Selective Demolition Activities : ...   \n",
       "\n",
       "   Target Date  Subcontractor  \n",
       "0          NaN            NaN  \n",
       "1          NaN            NaN  \n",
       "2          NaN            NaN  \n",
       "3          NaN            NaN  \n",
       "4          NaN            NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Dataset\n",
    "dataset = pd.read_excel(\"Dataset/Catalent BWI.xlsx\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S. No.</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spec #</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spec Name</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Para</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sub Section Heading</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Submittal Type</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Submittal Description</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target Date</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subcontractor</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0\n",
       "S. No.                 0\n",
       "Spec #                 1\n",
       "Spec Name              2\n",
       "Para                   3\n",
       "Sub Section Heading    4\n",
       "Submittal Type         5\n",
       "Submittal Description  6\n",
       "Target Date            7\n",
       "Subcontractor          8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column Name and Index of Column\n",
    "column_index_map = dict(zip(list(dataset.columns), range(len(dataset.columns))))\n",
    "pd.DataFrame.from_dict(column_index_map, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Dataset\n",
    "# Remove Unwanted Features from Frame\n",
    "#\n",
    "dataset.drop(columns = dataset.columns[[0, 1, 2, 3, 4, 5, 7, 8]], \n",
    "            axis = 1,\n",
    "            inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Existing Warranties : Remove, replace, patch, and repair materials and surfaces cut or \\ndamaged during selective demolition, by methods and with materials and using approved\\ncontractors so as not to void existing warranties. Notify warrantor before proceeding. Existing\\nwarranties include the following:\\n1. TPO Roofing System'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Submittal Description'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Rank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing Warranties : Remove, replace, patch, and repair materials and surfaces cut or damaged during selective demolition, by methods and with materials and using approved contractors so as not to void existing warranties.\n"
     ]
    }
   ],
   "source": [
    "# Text Rank \n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer \n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Creating text parser using tokenization\n",
    "parser = PlaintextParser.from_string(dataset['Submittal Description'][0], Tokenizer(\"english\"))\n",
    "\n",
    "# Summarize using sumy TextRank\n",
    "summarizer = TextRankSummarizer()\n",
    "summary =summarizer(parser.document, 1)\n",
    "\n",
    "text_summary=\"\"\n",
    "for sentence in summary:\n",
    "    text_summary+=str(sentence)\n",
    "\n",
    "print(text_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lex Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing Warranties : Remove, replace, patch, and repair materials and surfaces cut or damaged during selective demolition, by methods and with materials and using approved contractors so as not to void existing warranties.\n"
     ]
    }
   ],
   "source": [
    "# Lex Rank\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "summarizer_lex = LexRankSummarizer()\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer \n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Creating text parser using tokenization\n",
    "parser = PlaintextParser.from_string(dataset['Submittal Description'][0], Tokenizer(\"english\"))\n",
    "\n",
    "# Summarize using sumy TextRank\n",
    "summarizer = LexRankSummarizer()\n",
    "summary =summarizer(parser.document, 1)\n",
    "\n",
    "text_summary=\"\"\n",
    "for sentence in summary:\n",
    "    text_summary+=str(sentence)\n",
    "\n",
    "print(text_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing Warranties : Remove, replace, patch, and repair materials and surfaces cut or damaged during selective demolition, by methods and with materials and using approved contractors so as not to void existing warranties.\n"
     ]
    }
   ],
   "source": [
    "# LSA Summarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "summarizer_lex = LsaSummarizer()\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer \n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Creating text parser using tokenization\n",
    "parser = PlaintextParser.from_string(dataset['Submittal Description'][0], Tokenizer(\"english\"))\n",
    "\n",
    "# Summarize using sumy TextRank\n",
    "summarizer = LsaSummarizer()\n",
    "summary =summarizer(parser.document, 1)\n",
    "\n",
    "text_summary=\"\"\n",
    "for sentence in summary:\n",
    "    text_summary+=str(sentence)\n",
    "\n",
    "print(text_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK with Frequency Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\n",
      " Existing Warranties : Remove, replace, patch, and repair materials and surfaces cut or \n",
      "damaged during selective demolition, by methods and with materials and using approved\n",
      "contractors so as not to void existing warranties.\n"
     ]
    }
   ],
   "source": [
    "# NLTK with Frequency Count\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "data = dataset['Submittal Description'][0]\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "words = word_tokenize(data)\n",
    "\n",
    "freq_table = dict()\n",
    "for word in words:\n",
    "    word = word.lower()\n",
    "    if word in stopwords:\n",
    "        continue\n",
    "    if word in freq_table:\n",
    "        freq_table[word] += 1\n",
    "    else:\n",
    "        freq_table[word] = 1\n",
    "\n",
    "sentences = sent_tokenize(data)\n",
    "sentence_value = dict()\n",
    "\n",
    "for sentence in sentences:\n",
    "    for word, freq in freq_table.items():\n",
    "        if word in sentence.lower():\n",
    "            if sentence in sentence_value:\n",
    "                sentence_value[sentence] += freq\n",
    "            else:\n",
    "                sentence_value[sentence] = freq\n",
    "\n",
    "sum_value = 0\n",
    "for sentence in sentence_value:\n",
    "    sum_value += sentence_value[sentence]\n",
    "\n",
    "average = int(sum_value / len(sentence_value))\n",
    "\n",
    "summary = \"\"\n",
    "\n",
    "for sentence in sentences:\n",
    "    if(sentence in sentence_value) and (sentence_value[sentence] > (1.7 * average)):\n",
    "        summary = summary + \" \" + sentence\n",
    "\n",
    "print(\"Summary\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK Cosime Similarities and Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-63-a551f1baf72e>:30: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTICLE:\n",
      "Existing Warranties : Remove, replace, patch, and repair materials and surfaces cut or \n",
      "damaged during selective demolition, by methods and with materials and using approved\n",
      "contractors so as not to void existing warranties. Notify warrantor before proceeding. Existing\n",
      "warranties include the following:\n",
      "1. TPO Roofing System\n",
      "\n",
      "\n",
      "SUMMARY:\n",
      "Existing Warranties : Remove, replace, patch, and repair materials and surfaces cut or \n",
      "damaged during selective demolition, by methods and with materials and using approved\n",
      "contractors so as not to void existing warranties.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarities and Glove Embedding\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "## Data\n",
    "#\n",
    "data = dataset['Submittal Description'][0]\n",
    "\n",
    "## Sentence tokenization\n",
    "#\n",
    "sentences = sent_tokenize(data)\n",
    "\n",
    "## Word representation\n",
    "# \n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()\n",
    "\n",
    "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "stop_words = stopwords.words('english')\n",
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "\n",
    "## Sentence vectors\n",
    "#\n",
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
    "  if len(i) != 0:\n",
    "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "  else:\n",
    "    v = np.zeros((100,))\n",
    "  sentence_vectors.append(v)\n",
    "\n",
    "## Find similarities between sentences\n",
    "#\n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "for i in range(len(sentences)):\n",
    "  for j in range(len(sentences)):\n",
    "    if i != j:\n",
    "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "\n",
    "## Convert matrix into graph\n",
    "#\n",
    "import networkx as nx\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "## Final\n",
    "#\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "print(\"ARTICLE:\")\n",
    "print(data)\n",
    "print('\\n')\n",
    "print(\"SUMMARY:\")\n",
    "print(ranked_sentences[0][1])\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Gun - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google's T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KishanT\\Anaconda3\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:878: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "model = AutoModelWithLMHead.from_pretrained('t5-base', return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"Polishing Schedule : Submit plan showing polished concrete surfaces and schedule of polishing \n",
    "operations for each area of polished concrete before start of polishing operations. Include\n",
    "locations of all joints, including construction joints.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"Samples for Initial Selection : For each type of product requiring color selection.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"Mix Designs : For each type of mortar and grout. Include description of type and proportions of \n",
    "ingredients.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"\n",
    " Coordination Owners continuing occupancy portions existing building Owners partial occupancy completed Work\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(\"summarize: \" + data,\n",
    "                          return_tensors='pt',\n",
    "                          max_length=512,\n",
    "                          truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = model.generate(inputs, max_length = 4, min_length = 2, length_penalty=5., num_beams=2)\n",
    "summary = tokenizer.decode(summary_ids[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Work completed on\n"
     ]
    }
   ],
   "source": [
    "print(summary.replace('<pad>', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spacy NLargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, per):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc= nlp(text)\n",
    "    tokens=[token.text for token in doc]\n",
    "    word_frequencies={}\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if word.text not in word_frequencies.keys():\n",
    "                    word_frequencies[word.text] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.text] += 1\n",
    "    max_frequency=max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word]=word_frequencies[word]/max_frequency\n",
    "    sentence_tokens= [sent for sent in doc.sents]\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():                            \n",
    "                    sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent]+=word_frequencies[word.text.lower()]\n",
    "    select_length=int(len(sentence_tokens)*per)\n",
    "    summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)\n",
    "    final_summary=[word.text for word in summary]\n",
    "    summary=''.join(final_summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\"\"Samples for Initial Selection : For each type of product requiring color selection.\n",
    "\"\"\"\n",
    "summarize(data, 0.09)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Py Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysummarization.nlpbase.auto_abstractor import AutoAbstractor\n",
    "from pysummarization.tokenizabledoc.simple_tokenizer import SimpleTokenizer\n",
    "from pysummarization.abstractabledoc.top_n_rank_abstractor import TopNRankAbstractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Samples for Initial Selection : For each type of product requiring color selection.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples for Initial Selection : For each type of product requiring color selection.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Object of automatic summarization.\n",
    "auto_abstractor = AutoAbstractor()\n",
    "# Set tokenizer.\n",
    "auto_abstractor.tokenizable_doc = SimpleTokenizer()\n",
    "# Set delimiter for making a list of sentence.\n",
    "auto_abstractor.delimiter_list = [\".\", \"\\n\"]\n",
    "# Object of abstracting and filtering document.\n",
    "abstractable_doc = TopNRankAbstractor()\n",
    "# Summarize document.\n",
    "result_dict = auto_abstractor.summarize(document, abstractable_doc)\n",
    "\n",
    "# Output result.\n",
    "for sentence in result_dict[\"summarize_result\"]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Deep Learning Based Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n",
      "Downloading: 100%|██████████| 1.76k/1.76k [00:00<?, ?B/s]\n",
      "Downloading: 100%|██████████| 1.14G/1.14G [01:47<00:00, 11.4MB/s]  \n",
      "Downloading: 100%|██████████| 26.0/26.0 [00:00<00:00, 26.0kB/s]\n",
      "Downloading: 100%|██████████| 878k/878k [00:01<00:00, 568kB/s]  \n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 457kB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "summarization = pipeline(\"summarization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Samples for Initial Selection : For each type of product requiring color selection.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "clean_text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'samples for initial selection  for each type of product requiring color selection'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = clean_text.lower()\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Notify, Project)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "document = \"\"\"Notify warrantor on completion of selective demolition, and obtain documentation verifying\n",
    "that existing system has been inspected and warranty remains in effect. Submit\n",
    "documentation at Project closeout.\"\"\"\n",
    "doc = nlp(document)\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'samples initial selection   type product requiring color selection'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = re.sub(' +', ' ', ' '.join(clean_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:  samples initial\n"
     ]
    }
   ],
   "source": [
    "summary_text = summarization(last, max_length = 5, min_length = 3)[0]['summary_text']\n",
    "print(\"Summary:\", summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-3c792971b20f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# for kw in keywords:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# print(kw[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mkeywords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# pip install yake\n",
    "import yake\n",
    "\n",
    "document = \"\"\"No\"\"\"\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "language = \"en\"\n",
    "max_ngram_size = 3\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 1\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan = language, \n",
    "                n = max_ngram_size, \n",
    "                dedupLim = deduplication_threshold, \n",
    "                top = numOfKeywords, \n",
    "                features = None)\n",
    "keywords = custom_kw_extractor.extract_keywords(document)\n",
    "# keywords[0][0]\n",
    "# for kw in keywords:\n",
    "    # print(kw[0])\n",
    "keywords[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34b8bd46980c0826764a5b36ed1c6c81aa64b95fe6c6003f76753dd59089c49f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
